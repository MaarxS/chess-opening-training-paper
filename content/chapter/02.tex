%!TEX root = ../../main.tex

\chapter{Theoretische Grundlagen}
In den folgenden Kapiteln sollen die Theoretischen Grundlagen erläutert werden. Dafür wird zuerst betrachtet, wie das Lernen im Allgemeinen funktioniert und wie diese Erkenntnisse auf das Lernen von Eröffnungen angewendet werden können.
In dem darauffolgenden Kapitel wird betrachtet welche Erkenntnisse es konkret im Bereich der Schachpsychologie gibt. Dafür wird betrachtet, wie Schachspieler nach heutigem Wissensstand lernen und spielen.

\section{Allgemeines Lernen}
Über das Thema Lernen gibt es mehrere Theorien und Modelle. Aus diesen Theorien kann man auch teilweise Schlüsse ziehen, wie man das Erlernen von Schacheröffnungen gestalten kann.
Im Folgenden werden die Theorie des Behaviorismus, des Lernen am Modell und die strukturgenetische Lerntheorie kurz beschrieben.
Danach wird auch die Funktionsweise unseres Gedächtnisses betrachtet.

\subsection{Behaviorismus}
Der Behaviorismus basiert auf tatsächlich beobachtbarem Verhalten. Er besagt, dass jeder Mensch durch seine Umwelt beeinflusst wird. Das bedeutet, dass man das Verhalten auch verändern kann durch Stimulation, primär durch positive Verstärkung und negative Verstärkung. Bei gutem Verhalten ist also Belohnung und Lob sinnvoll und bei schlechtem Verhalten können Strafen eingesetzt werden. Vor allem die positive Verstärkung sorgt dafür, dass eine Person extrinsische Motivation bekommt und auch gerne weiterlernt.\cite{kron_grundwissen_2024}

Bezogen auf eine Schachanwendung kann man bei einem guten Zug positives Feedback geben zum Beispiel durch Lob oder durch positive Symbole, Farben und Animationen. Man könnte auch Belohnungen einführen, wenn der Spieler mehrere aufeinanderfolgende Tage Übungen durchgeführt hat, oder sich verbessert.

\subsection{Lernen am Modell}
Eine weitere Theorie ist, dass Menschen lernen am Modell. Das bedeutet, dass sie durch Beobachtung eines Modells neue Dinge erlernen können. Dieses Modell kann zum Beispiel eine andere Person sein.
Dieser Effekt tritt insbesondere dann auf, wenn die beobachtete Aktion in einem positiven Ergebnis resultiert. Dann ist die Wahrscheinlichkeit am höchsten, dass diese Aktion in einer ähnlichen Situation nachgeahmt wird.\cite{kron_grundwissen_2024}

In einer Schachanwendung kann man dieses Wissen so anwenden, dass man dem Spieler einen Zug vorführt und ihm dadurch zeigt,
%wie dieser zu einer vorteilhaften Position führen kann.
wie dieser einen Vorteil bringt.
In einer ähnlichen Position kann der Spieler sich dann eher an den Zug erinnern und ihn durchführen.

\subsection{Strukturgenetische Lerntheorie}
Die strukturgenetische Lerntheorie geht davon aus, dass ein Mensch am besten lernt, wenn er selbst verschiedene Handlungswege ausprobieren kann. Es geht darum, Personen in Herausforderungen zu versetzen und sie durch selbstständige Entdeckung etwas lernen zu lassen. Voraussetzung dafür ist, dass die Aktionen im Nachhinein reflektiert werden und dass die Person auch die notwendigen Kenntnisse und Mittel dafür hat. Dadurch kann sich dann Erfahrung bilden, denn \enquote{Erfahrung ist [...] immer reflektiertes oder durch Reflexion bestimmtes Tun.}\cite{kron_grundwissen_2024}

In einer Schachanwendung kann das durch die Implementierung von Rätseln geschehen. Man kann dem Spieler eine bestimmte Anfangsposition geben in welcher nur eine Auswahl an Zügen zu einer guten Position führen. Es wäre auch möglich ein Analysetool zur Verfügung zu stellen, mit welchem ein vergangenes Spiel reflektiert werden kann. So können gute und schlechte Züge identifiziert werden und der Spieler kann lernen, welche Züge gut funktioniert haben und welche nicht.

\subsection{Funktionsweise des Gedächtnis}\label{gedächtnis}
Für das Erlernen von Schacheröffnungen ist es auch sinnvoll zu verstehen, wie das menschliche Gedächtnis funktioniert. Das Gedächtnis bildet die Grundlage des Lernens. Es wird als Netzwerk verstanden, das mit den Wahrnehmungsprozessen verbunden ist. Eine Besonderheit des menschlichen Gedächtnisses ist, dass sich Erinnerungen durch wiederholtes Erinnern verändern. Jedes mal, wenn man sich an etwas erinnert verändern sich die Erinnerungen ein wenig und es wird eventuell mehr Kontext hinzugefügt. Das bedeutet, dass eine Erinnerung, immer subjektiver wird und somit auch verfälscht werden kann. Außerdem kann es auch zum Vergessen kommen, indem vorhandene Erinnerungen überschrieben werden. Das Abspeichern von Informationen kann unterstützt werden durch verschiedene Lerntechniken. Dazu gehört zum Beispiel das Wiedergeben von Inhalten, Gruppieren von Inhalten oder Herausfiltern von Hauptideen. Gespeichert werden Inhalte auf drei unterschiedliche Arten. Das sensorische Gedächtnis ist für Inhalte zuständig, die jetzt im Moment wahrgenommen werden. Damit sind äußere Reize und innere Zustände gemeint. Das Kurzzeitgedächtnis verarbeitet aktuelle Informationen zu Wissen. Seine Kapazität ist allerdings stark begrenzt. Deshalb können Inhalte nicht lange im Kurzzeitgedächtnis behalten werden, sie durch neue Inhalte ersetzt und verdrängt. Im Langzeitgedächtnis werden Inhalte des Kurzzeitgedächtnisses übernommen. Dieser Prozess kann durch Lerntechniken unterstützt werden. Das Langzeitgedächtnis hat eine sehr große Kapazität und behält Informationen lange um später darauf zurückgreifen zu können. Hier wird wiederum zwischen dem deklarativen und dem prozeduralen Gedächtnis unterschieden. Das deklarative Gedächtnis ist zuständig für bewusst abrufbare Inhalte, wie persönliche Erlebnisse, Fakten und allgemein Inhalte die mit Worten kommuniziert werden können. Das prozedurale Gedächtnis enthält auch Wissen, das nicht unbedingt bewusst wahrgenommen wird. Dort sind auch zum Beispiel motorische Kenntnisse gespeichert.
\cite{kron_grundwissen_2024}

Aus diesen Erkenntnissen kann man schlussfolgern, dass es besonders wichtig ist Inhalte zu wiederholen, damit sich Gelerntes nicht verfälscht und es nicht vergessen wird. Im Schach sollte man um Eröffnungen zu erlernen diese also häufig betrachten. Durch das eigene Spielen und Erinnern kann das Übergehen dieser Eröffnungen in das Langzeitgedächtnis unterstützt werden. Bestimmte Variationen, welche seltener vorkommen sollten auch wiederholt werden, um ein Vergessen, Überschreiben oder Verfälschen zu verhindern.

\section{Schachpsychologie}% Psychologie des Eröffnungen Lernens
In der Schachpsychologie wurde von unterschiedlichen Personen bereits einige gemeinsame Beobachtungen gemacht. Gobet und Jansen haben in \cite{gobet_training_2006} die folgende Kernaussagen gesammelt:

\begin{enumerate}
    \item Ein Schachspieler hat einen hocheffizienten Überblick. Er erkennt zentrale Elemente einer Position sehr schnell.
    \item Schachspieler können sich Schachpositionen und Spiele außergewöhnlich gut merken. Diese Fähigkeit ist außerhalb von Schach nicht erkennbar.
    \item Ihr Wissen besteht aus mehreren Ebenen, im speziellen die niedrige Ebene, welche aus Muster von Figuren besteht und eine hohe konzeptionelle Ebene, welche sich mit Plänen und Bewertungen von Positionen befasst.
    \item Die Spieler suchen sehr selektiv nach guten Zügen. Es werden nur ganz bestimmte Pfade durchdacht und andere sehr schnell verworfen.
    \item Es gibt keinen Unterschied in dem Suchalgorithmus eines Expertenspielers und eines Großmeisters.
    \item Meister verlieren in simultanen Spielen relativ wenig ihrer Fähigkeiten.
\end{enumerate}

\subsection{Die Template Theorie}
Aufschluss über diese Beobachtungen soll die Template Theorie geben, welche von Gobet und Simon in \cite{gobet_templates_1996} beschrieben wird. Die Template Theorie berücksichtigt, dass das kognitive System der Menschen aus drei Hauptbestandteilen besteht, wie in \autoref{gedächtnis} beschrieben.
Das sensoriesche Gedächtnis wird hier räumlich-zeitlicher Speicher genannt und es wird auch das stark begrenzte Kurzzeitgedächtnis und das Langzeitgedächtnis näher betrachtet.
Bei dem Langzeitgedächtnis wird wieder zwischen deklarativem und prozeduralem Wissen unterschieden.
\cite{gobet_training_2006}

Bei dem deklarativen Wissen in Bezug auf Schach wird vermutet, dass Chunks eine große Rolle spielen. Chunks sind kleine Muster, also eine Anordnung von einigen Spielfiguren, die öfter bei einem Schachspiel vorkommen. Es wird geschätzt, dass das Kurzzeitgedächtnis Platz für ungefähr sieben Chunks hat. Ein großer Unterschied zwischen geübten und weniger geübten Schachspielern ist also, wie viele Chunks sie in ihrem Langzeitgedächtnis haben. Je besser ein Schachspieler ist, desto mehr und desto größere Chunks befinden sich in seinem Langzeitgedächtnis. Es wird vermutet, dass ein geübter Spieler ungefähr 50.000 Chunks in seinem Langzeitgedächtnis hat. Wenn ein Muster erkannt wird, muss im Kurzzeitgedächtnis nur ein Zeiger auf den Chunk im Langzeitgedächtnis behalten werden. Auf diese Weise können geübte Schachspieler sich sehr effizient Schachpositionen merken. Das erklärt auch, warum sich diese Fähigkeit nicht auf Bereiche außerhalb des Schachs auswirkt. Selbst zufällig angeordnete Positionen können sich gute Spieler schlecht merken. Erweitert wird diese Theorie durch Templates, also Vorlagen. Templates sind spezielle Chunks, welche Platzhalter beinhalten für bestimmte Figuren. So können durch Templates noch mehr Positionen abgedeckt werden, als es durch einfache Chunks möglich ist. Viele Chunks und Templates reduzieren die Notwendigkeit voraus zu schauen. Wenn bekannten Chunks begegnet wird, ist auch klar, wie gut diese Position zu bewerten ist und welche Züge in Frage kommen.
\cite{gobet_templates_1996}

Dieses Wissen, was als nächstes getan werden kann, wird von Gobet und Jansen unter dem prozedurales Wissen eingeordnet. Es wird durch sogenannte Produktionen abgespeichert. Produktionen sind Wissenseinheiten, welche aus Bedingung und Aktion bestehen. Ein Beispiel wäre, \enquote{Wenn es eine Linie ohne Figuren gibt und du einen Turm besitzt, dann platziere den Turm auf dieser Reihe.} Durch diese Produktionen können Spieler schnell Entscheidungen treffen. Dieser Mechanismus kann bewusst oder unbewusst stattfinden und wird oft als Intuition verstanden.
\cite{gobet_training_2006}

\subsection{Erlernen von Eröffnungen}
Das Lernen von Eröffnungen ist sehr zeitaufwendig. Es wird vermutet, dass sieben bis 10 Sekunden benötigt werden um einen Chunk im Langzeitgedächtnis zu lernen. Außerdem wird es passieren, dass man Dinge wieder vergisst. Deshalb wird empfohlen sich auf wenige Eröffnungen zu konzentrieren. Für diese Eröffnungen kann man dann die häufigen Variationen und Positionen lernen. Man sollte so viele Variationen lernen, dass man auf die typischen Züge des Gegners reagieren kann. Indem man die Anzahl der Eröffnungen limitiert, erhöht man die Wahrscheinlichkeit, dass gelernten Chunks und Templates in einem richtigen Spiel begegnet wird. Beim Erlernen von Eröffnungen ist es außerdem wichtig eine Balance zwischen Auswendiglernen und Verständnis zu finden. Um sich die Abfolge der Züge zu merken muss man selbstverständlich viel Auswendiglernen, allerdings ist das Verständnis der Hauptideen nützlich um auch in unbekannten Situationen sinnvolle Züge zu finden. Dazu kommt, dass das Herausfiltern von Hauptideen, wie in \autoref{gedächtnis} beschrieben auch eine Lerntechnik ist, die beim Übertragen von Inhalten in das Langzeitgedächtnis helfen kann. Man sollte die Eröffnungen auch aus unterschiedlichen Perspektiven betrachten und Verknüpfungen zu Mittelspielen und Endspielen herstellen. Das soll die Erstellung von Templates beschleunigen. Ein zentraler Ort um die Eröffnungen nachzuschauen ist auch sinnvoll. Damit kann man sein Wissen einfach erneuern und verhindert, dass die Erinnerungen Verblassen oder sich verfälschen, wie in \autoref{gedächtnis} beschrieben.
\cite{gobet_training_2006}

Für eine Schachanwendung bedeutet das, dass man einen Spieler dazu ermutigen sollte eine Eröffnung und ihre Variationen im Detail zu erlernen. Man sollte die Spieler auch dazu bringen die Eröffnungen oft zu betrachten. Hier kann die Schachanwendung selbst der zentrale Ort werden um Eröffnungen nachzuschauen und seine Erinnerung aufzufrischen. Um die Verknüpfungen zwischen Eröffnungen und Mittelspielen herzustellen kann man dem Spieler die Möglichkeit geben, von einer bestimmten Eröffnung als Startpunkt gegen einen Computergegner zu spielen. Für die Verknüpfung von Eröffnungen und Endspielen kann die von Gobet und Jansen in \cite{gobet_training_2006} vorgeschlagene Dekompositionsmethode verwendet werden. Hierbei werden nach der Eröffnug fast alle Figuren außer den Bauern und Königen vom Spielfeld entfernt. Dadurch bekommt man ein besseres Gefühlt dafür, welche Seite eine bessere Position hat. Man nach der Dekomposition den Spieler auch von dieser Position aus gegen einen Computer spielen lassen. Das Verstehen der Hauptideen von Eröffnungen kann erreicht werden durch beschreibende Infotexte. Auf diese Weise werden alle zuvor beschriebenen Bereiche abgedeckt. 

Auch Gobet und Jansen sehen das Potential für computergestütztes Lernen. Sie erwähnen die Nützlichkeit von Spieldatenbanken, Computergegnern und Analysen. Sie beschreiben auch, dass zukünftige Computerprogramme noch weitergehen können, indem sie wie ein Trainer personalisiert Übungsmaterial heraussuchen basierend auf den Stärken und Schwächen des Spielers.
\cite{gobet_training_2006} 

Gusev verwendet auch Computer um auf Schacheröffnungen vorzubereiten. In \cite{gusev_using_2021} zählt er verschiedene Softwareprodukte auf, die beim Erlernen von Schacheröffnungen helfen können. Darunter sind Schachengines, Spieldatenbanken, Cloud Services und Puzzles. Er präsentiert eine computerbasierte Herangehensweise um Eröffnungen zu trainieren. Dafür hat er mithilfe von Schachengines, Spieldatenbanken und einem Cloud Service beliebte Eröffnungsvarianten identifiziert und sortiert. Mögliche Fortführungen wurden durch eine Schachengine mit Eröffnungsbuch herausgefunden. Das Resultat war ein nützliches Werkzeug um Studenten Eröffnungen zu zeigen.

\section{Funktionsweise von Schachengines}
Seit langer Zeit wird die Fähigkeit Schach zu spielen mit Intelligenz verbunden.
Daher gab es auch großes Interesse daran Computern das Spielen von Schach beizubringen.
Es wurden Preise ausgeschrieben für die Programme, die als erstes eine bestimmte Wertung erreichen oder den Weltmeister besiegen. Das erste Mal, dass ein Computer einen Weltmeister besiegte war im Jahr 1997. Das von IBM entwickelte Programm namens \enquote{Deep Blue} besiegte den aktuellen Weltmeister Garry Kasparov. Seitdem ist die Lücke zwischen Menschen und Computern immer größer geworden und heutzutage kann kein Mensch mehr gegen die besten Schachcomputer gewinnen.
\cite{vjekoslav_nemec_history_2019}

Computerprogramme, die Schach spielen werden typischerweise Schachengines genannt. Es gibt Formen von Schachengines. Die eine nutzt den Minimax"=Algorithmus in Verbindung mit Alpha-Beta-Pruning. Das ist auch die Art von Schachengines, die zuerst entwickelt wurden. Eine weitere Form wurde in den letzten Jahren entwickelt. Diese arbeiten mit reinem Reinforcement Learning und erlernen das Spiel, indem sie zahlreiche Spiele gegen sich selbst spielen. In den Folgenden zwei Kapiteln werden beide Herangehensweisen näher erläutert.

\subsection{Alpha-Beta-Pruning}
Der Alpha-Beta-Pruning Algorithmus kann genutzt werden um ein Spiel mit zwei Spielern, die gegensätzliche Ziele verfolgen, zu spielen. Der Algorithmus baut auf dem Minimax Algorithmus auf und optimiert diesen durch Kürzen (Pruning) des Suchbaums.

Für den Minimax Algorithmus wird ein Spiel anhand von Positionen $p$ definiert und anhand von Regeln, wie von einer Position in eine andere Übergegangen werden kann. Wenn es keine weiteren legalen Züge gibt, dann ist eine Terminalposition erreicht. Diese Struktur kann als sogenannter Spielbaum dargestellt werden. Die Positionen bilden die Knoten und wenn es ein Übergang von einer Position in eine andere existiert, wird eine Kante eingezeichnet. Falls eine Position schon einmal erreicht wurde darf allerdings keine Kante hinzugefügt werden um Zyklen zu verhindern. Bei diesem Baum ist $h$ die Tiefe des Baumes und $d$ ist die Anzahl an Verzweigungen an einem Knoten. Die Terminalpositionen bilden die Blattknoten.

Jeder Terminalposition wird ein Wert zugewiesen durch die Evaluationsfunktion $f(p)$. Für einen Spieler ist der Wert des Spiels $f(p)$ und für den anderen der Wert $-f(p)$. Beide Spieler möchten jeweils ihren Wert maximieren. Alternativ kann man auch formulieren, dass ein Spieler den Wert von $f(p)$ maximieren will und der Gegner will ihn minimieren.
Beide Herangehensweisen sind equivalent. Wenn $p$ eine Position ist, bei der es es $d$ legale Züge $p_1,\ldots,p_d$ mit $d > 1$ gibt, dann ist es die Aufgabe des Algorithmus den besten Zug auszuwählen. Der beste Zug ist der Zug, bei dem beste Wert für den Spieler am Zug erreicht wird. Es wird davon ausgegangen, dass der Gegner auch jeweils die Züge auswählt, die für ihn zu dem besten Wert führen. Es sei $F(p)$ der größtmögliche Wert, der von Position $p$ aus erreichbar ist gegen einen optimal spielenden Gegner. Die Funktion $F(p)$ wird definiert durch:
\begin{equation}
    F(p) = 
    \begin{cases}
        f(p) & \text{für } d = 0 \\
        max(-F(p_1),\ldots,-F(p_d)) & \text{für } d > 0
    \end{cases}
\end{equation}

Mit Pseudocode kann diese Funktion folgendermaßen abgebildet werden:
\begin{lstlisting}
function F(p: Position): Int {
    ps = findSuccessors(p)
    if (d == 0) {
        return f(p)
    }
    m = -infinity
    for (pi in ps) {
        t = -F(pi)
        m = max(t, m)
    }
    return m
}
\end{lstlisting}

Dieser Algorithmus durchsucht alle Möglichen Fortsetzungen von $p$ und findet den besten Zug mit dem besten Wert. 
\cite{knuth_analysis_1975}

Es ist allerdings nicht immer notwendig alle Fortsetzungen zu zu überprüfen. Oft kann erkannt werden, dass eine bestimmte Zugfolge keinen besseren Wert mehr erreichen kann und daher ignoriert werden kann.
Wenn man zum Beispiel eine Zugfolge herausgefunden hat, die mit dem Wert 3 bewertet wird und man über eine andere Zugfolge sagen kann, dass diese nicht besser als 3 wird, muss man diese nicht weiter nachverfolgen. Wenn also $-F(p_1) = 3$ ist, dann ist $F(p) \geq 3$ und jeder Pfad mit $-F(p_i) \leq 3$ ist kann ignoriert werden, weil der Gegner den Zug so wählen wird,  dass entweder derselbe Wert oder ein für uns kleinerer Wert erzielt wird. Indem man den aktuell höchsten Wert zwischenspeichert, ist es möglich bestimmte Suchzweige auszulassen.
\cite{knuth_analysis_1975}

\missingfigure{Beispiel}

Dieses Vorgehen kann weiter verbessert werden indem man den höchsten und den kleinsten Wert zwischenspeichert. Dieses Vorgehen wird Alpha-Beta-Pruning genannt. Knuth und Moore definieren die Funktion folgendermaßen:

\begin{equation}
    \begin{array}{ll}
        F2(p, \alpha, \beta) \leq \alpha & \text{für }F(p) \leq \alpha \\
        F2(p, \alpha, \beta) = F(p) & \text{für }\alpha < F(p) < \beta \\
        F2(p, \alpha, \beta) \geq \beta & \text{für }F(p) \geq \beta
    \end{array}
\end{equation}

In Pseudocode kann der Algorithmus folgendermaßen umgesetzt werden:
\begin{lstlisting}
function F2(p: Position, alpha: Int, beta: Int): Int {
    ps = findSuccessors(p)
    if (d == 0) {
        return f(p)
    }
    m = alpha
    for (pi in ps) {
        t = -F2(pi, -beta, -m)
        m = max(t, m)
        if (m >= beta) break
    }
    return m
}
\end{lstlisting}
\cite{knuth_analysis_1975}

\missingfigure{Beispiel}

Für diesen Algorithmus können weitere Verbesserungen vorgenommen werden. Einen großen Faktor spielt die Reihenfolge, in der die Züge untersucht werden. Bei einer optimalen Reihenfolge muss der Alpha-Beta-Algorithmus $d^{\lceil h/2 \rceil} + d^{\lfloor h/2 \rfloor} - 1$ Terminalpositionen bewerten. \cite{knuth_analysis_1975} Das ist dann der Fall, wenn beim Durchsuchen des Baumes immer der richtige Zug ausgewählt wird, sodass er maximal gekürzt werden kann. Der Algorithmus kann dahingehend verbessert werden, dass Züge, die mit hoher Wahrscheinlichkeit einen Vorteil bringen zuerst betrachtet werden. Im Schach können zum Beispiel Züge in welchen eine Figur geschlagen wird zuerst betrachtet werden, da diese oft einen Vorteil bringen. Andere Züge, wie zum Beispiel Damenopfer können später betrachtet werden.
Um die Reihenfolge der Züge für eine tiefere Suche zu verbessern, wird häufig zunächst eine flachere Suche durchgeführt. Dieses Verfahren wird als iterative Vertiefung (iterative deepening) bezeichnet. 
Eine optimale Reihenfolge kann man dadurch jedoch nicht garantieren. Wie in der Gleichung zu sehen ist steigt die Anzahl der zu besuchenden Knoten selbst im Optimalfall exponentiell. Es kann also nur eine bestimmte Anzahl an Zügen in die Zukunft geschaut werden. Für viele Spiele, einschließlich Schach, ist die Gesamtzahl der möglichen Züge in einem Spiel jedoch so hoch, dass es unmöglich ist, alle Szenarien vollständig zu analysieren. Aus diesem Grund wird eine bestimmte Suchtiefe festgelegt, ab der die Positionen wie Terminalpositionen behandelt werden.\cite{knuth_analysis_1975} Es muss also für diese Position auch eine Evaluationsfunktion $f$ existieren. In der naivsten Form kann dafür die Anzahl der Figuren der Spieler genommen werden. In komplexeren Schachengines werden Neurale Netze verwendet um Positionen zu bewerten. Eine Tabelle mit bereits berechneten Positionen kann Berechnungen auch beschleunigen. Für Eröffnungen und Endspiele können Eröffnungsbücher und vorgerechnete Endspiel Tabellen verwendet werden.\cite{silver_mastering_2017}

\subsection{Reinforcement Learning}
Der klassische Alpha-Beta-Pruning Algorithmus kann nur durch viel spielbezogenes Wissen verbessert werden. Im Gegensatz dazu kann im Reinforcement Learning ein Computer ein Spiel ohne Vorwissen erlernen, indem er wiederholt Spiele gegen sich selbst simuliert. Das bekannteste Beispiel dafür ist AlphaZero, ein Algorithmus, der nach wenigen Stunden Training die besten Programme für Schach, Shogi und Go schlagen konnten. \cite{silver_mastering_2017} In diesem Kapitel wird die Funktionsweise von AlphaZero näher erklärt.

AlphaZero basiert auf dem Algorithmus AlphaZero Go, welcher der erste ist, der Go auf einem \enquote{superhuman level}\cite{silver_mastering_2017-1} spielen kann. Einige Verbesserungen, die speziell für Go waren, wurden dafür weggelassen.\cite{silver_mastering_2017} Beide Algorithmen verwenden ein tiefes Neurales Netz\todo{"Deep Neural Network"\\besser?} im Zusammenhang mit der \ac{MCTS}.

Bei \ac{MCTS} wird ein Suchbaum inkrementell aufgebaut bis ein Rechenlimit erreicht wird. Dieses Rechenlimit wird meistens durch eine Zeitbegrenzung realisiert. Es wird also für eine bestimmte Zeit ein Suchbaum aufgebaut, dann abgebrochen und der bis dahin beste gefundene Wert ausgegeben. Dieses Verfahren ist sehr effizient für Bäume mit hoher Verzweigung. In diesem Verfahren werden bereits bekannte Knoten durch das Baumvorgehen\todo{"Tree Policy" besser?} ausgewählt und sobald ein Knoten mit unbekannten Kindesknoten erreicht wird, wird das Standardvorgehen\todo{"Default Policy"\\besser?} angewendet. Bereits besuchte Knoten besitzen eine Bewertung und die Anzahl, wie oft sie besucht wurden. Ein Baumvorgehen muss die Knoten so auswählen, dass vielversprechende Knoten besucht werden und auch neue Pfade erkundet werden. Es gilt also eine Balance zu finden zwischen gut bewerteten Knoten und selten besuchten Knoten. Der Erfolgt von \ac{MCTS} hängt zu großen Teilen von diesem Baumvorgehen ab. Ein Verfahren, was sich als erfolgreich herausgestellt hat ist der \enquote{upper confidence bound for trees (UCT)} Algorithmus, welcher hier nicht näher erläutert wird. Bei einem Knoten mit unbekannten Kindesknoten wird ein Spiel simuliert, bis ein Terminalknoten erreicht wird. Der Terminalknoten kann mit einem Belohnungswert bewertet werden. Dieser Belohnungswert wird zurückpropagiert und die Bewertungen der vorhergegangen Knoten werden angepasst. Die Simulation des Spiels wird durch das Standardvorgehen bestimmt. Das kann im einfachsten Fall eine gleichmäßige Zufallsverteilung sein.
\cite{browne_survey_2012}

Bei AlphaZero wird \ac{MCTS} mit einem tiefen Neuralen Netz $f_\theta$ mit den Paramtern $\theta$ verwendet. Dieses Netz bekommt die aktuelle Position $s$ als Eingabe und gibt einen Vektor $p$ und einen Skalar $v$ aus $(p, v) = f_\theta(s)$. Der Vektor $p$ gibt die Wahrscheinlichkeit an, dass ein bestimmter Zug ausgewählt wird. Der Skalar $v$ schätzt die Wahrscheinlichkeit, dass der Spieler am Zug mit der aktuellen Position gewinnen wird. AlphaZero wird durch Reinforcement Learning trainiert indem er wiederholt gegen sich selbst spielt. Für jede Position $s$ wird eine \ac{MCTS} Suche durchgeführt, welche durch das Neurale Netz $f_\theta$ geführt wird. Die Ausgabe dieser Suche sind die Zugwahrscheinlichkeiten $\pi$ und der Gewinner $z$. Diese Wahrscheinlichkeiten sind deutlich besser, als die Ausgaben des Neuralen Netzes. Die Parameter des neuralen Netzes werden angepasst, sodass der Unterschied zwischen $(p, v) = f_\theta(s)$ und $(\pi, z)$ möglichst gering wird. Konkret werden die Parameter $\theta$ durch den Gradientenabstieg auf einer typischen Verlustfunktion angepasst.
\cite{silver_mastering_2017-1}\cite{silver_mastering_2017}

Der beschriebene Algorithmus wurde jeweils für Schach, Shogi und Go trainiert. In Schach und Shogi spielte AlphaZero nach weniger als einem Tag Training besser als die bis dahin führenden Programme. Im Vergleich zu Alpha-Beta-Pruning Algorithmen, wie zum Beispiel Stockfish durchsucht AlphaZero deutlich weniger Pfade pro Sekunde. Das ganze wird dadurch kompensiert, dass AlphaZero bei der Auswahl der zu untersuchenden Pfade selektiver vorgeht. Es wurde auch gezeigt, dass AlphaZero effizienter skaliert mit mehr Bedenkzeit. Des weiteren werden Fehler in der Schätzung von Positionen durch \ac{MCTS} geschwächt, während sie bei Alpha-Beta-Pruning Algorithmen bis zur Wurzel wandern.
\cite{silver_mastering_2017}

\section{Schnittstellen}
Damit Schachengines in andere Programme eingebunden werden können sind Schnittstellen notwendig. In den folgenden Kapiteln wird beschrieben, wie die Schnittstelle zu einer Schachengine meistens gestaltet wird und wie Schnittstellen zwischen einer Weboberfläche und dem Backend gebildet werden können.

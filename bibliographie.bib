
@article{gusev_using_2021,
	title = {Using {Modern} {Chess} {Software} for {Opening} {Preparation}},
	url = {https://eric.ed.gov/?id=ED617407},
	abstract = {A wide variety of modern chess software products is available to the modern professional and amateur chess players alike, helping them improve their chess skills and prepare for online and traditional tournaments. These products include chess user interfaces (UIs), traditional Alpha-Beta (AB) and emergent Neural Network (NN) chess engines, game databases, opening databases and electronic books, chess-specific cloud services, tournament broadcast tools, online tutorials, tactical problem collections, and endgame tablebases (EGTBs). All of these tools except the last two categories can be used to work on opening preparation, an important component of chess training. In this paper, the author presents his computer-based approach to opening preparation tested in chess classes at the Russian School of Indiana for advanced beginner players. The materials used to develop the approach included game openings from the games played in the Free Open-Source Chess Engine Contest (FOSCEC) broadcast online by the author’s CIT students at Purdue Polytechnic Columbus. We will discuss the choices of tools and equipment, how the more popular and/or promising opening variations were identified and analyzed, the lessons learned, and the future work.},
	language = {en},
	author = {Gusev, Dmitri A},
	year = {2021},
	file = {PDF:C\:\\Users\\marvi\\Zotero\\storage\\LFWFP4VF\\Gusev - 2021 - Using Modern Chess Software for Opening Preparation.pdf:application/pdf},
}

@article{gobet_training_2006,
	title = {Training in chess: {A} scientific approach},
	journal = {Chess and education},
	author = {Gobet, Fernand and Jansen, Peter},
	month = jan,
	year = {2006},
	file = {PDF:C\:\\Users\\marvi\\Zotero\\storage\\FSWE5CKU\\Gobet und Jansen - Training in chess A scientific approach.PDF:application/pdf},
}

@book{kron_grundwissen_2024,
	address = {Stuttgart},
	edition = {7., vollständig überarbeitete und erweiterte Auflage},
	series = {{UTB}},
	title = {Grundwissen {Didaktik}},
	isbn = {978-3-8252-8802-0 978-3-8385-8802-5 978-3-8463-8802-0},
	abstract = {Das Standardwerk zur Didaktik für Studierende erziehungswissenschaftlicher Fächer. Dieses Lehrbuch bietet eine verständlich geschriebene, wissenschaftliche Grundlage der Didaktik für Schule und außerschulische Bildungsbereiche. Zentral sind die Kapitel über didaktische Theorien, Modelle und Konzepte sowie über Lerntheorien. Das Lehrbuch eignet sich hervorragend als Informationsquelle und Nachschlagewerk für Prüfungsvorbereitungen aller Fachbereiche mit hohem Anteil an Didaktik},
	language = {ger},
	number = {8073},
	publisher = {Ernst Reinhardt Verlag},
	author = {Kron, Friedrich W. and Jürgens, Eiko and Standop, Jutta},
	year = {2024},
	doi = {10.36198/9783838588025},
	file = {2024-3-lerntheorien-und-modelle-im-kontext-von-lehren-und-lernen:C\:\\Users\\marvi\\Zotero\\storage\\5JLZP23L\\2024-3-lerntheorien-und-modelle-im-kontext-von-lehren-und-lernen.pdf:application/pdf},
}

@article{chesscom_chesscom_2022,
	title = {Chess.com hat 100 {Millionen} {Mitglieder}},
	url = {https://www.chess.com/de/article/view/chess-com-hat-100-millionen-mitglieder},
	urldate = {2024-11-03},
	author = {Chess.com},
	month = dec,
	year = {2022},
	file = {PDF:C\:\\Users\\marvi\\Zotero\\storage\\695M6XLH\\Chess.com - 2022 - Chess.com hat 100 Millionen Mitglieder.pdf:application/pdf},
}

@article{gobet_templates_1996,
	title = {Templates in {Chess} {Memory}: {A} {Mechanism} for {Recalling} {Several} {Boards}},
	volume = {31},
	issn = {0010-0285},
	url = {https://www.sciencedirect.com/science/article/pii/S0010028596900110},
	doi = {https://doi.org/10.1006/cogp.1996.0011},
	abstract = {This paper addresses empirically and theoretically a question derived from the chunking theory of memory (Chase \& Simon, 1973a, 1973b): To what extent is skilled chess memory limited by the size of short-term memory (about seven chunks)? This question is addressed first with an experiment where subjects, ranking from class A players to grandmasters, are asked to recall up to five positions presented during 5 s each. Results show a decline of percentage of recall with additional boards, but also show that expert players recall more pieces than is predicted by the chunking theory in its original form. A second experiment shows that longer latencies between the presentation of boards facilitate recall. In a third experiment, a Chessmaster gradually increases the number of boards he can reproduce with higher than 70\% average accuracy to nine, replacing as many as 160 pieces correctly. To account for the results of these experiments, a revision of the Chase–Simon theory is proposed. It is suggested that chess players, like experts in other recall tasks, use long-term memory retrieval structures (Chase \& Ericsson, 1982) or templates in addition to chunks in short-term memory to store information rapidly.},
	number = {1},
	journal = {Cognitive Psychology},
	author = {Gobet, Fernand and Simon, Herbert A.},
	year = {1996},
	pages = {1--40},
	file = {PDF:C\:\\Users\\marvi\\Zotero\\storage\\E8MG5VR4\\Gobet und Simon - 1996 - Templates in Chess Memory A Mechanism for Recalling Several Boards.pdf:application/pdf},
}

@article{knuth_analysis_1975,
	title = {An analysis of alpha-beta pruning},
	volume = {6},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/0004370275900193},
	doi = {https://doi.org/10.1016/0004-3702(75)90019-3},
	abstract = {The alpha-beta technique for searching game trees is analyzed, in an attempt to provide some insight into its behavior. The first portion of this paper is an expository presentation of the method together with a proof of its correctness and a historical discussion. The alpha-beta procedure is shown to be optimal in a certain sense, and bounds are obtained for its running time with various kinds of random data.},
	number = {4},
	journal = {Artificial Intelligence},
	author = {Knuth, Donald E. and Moore, Ronald W.},
	year = {1975},
	pages = {293--326},
	file = {PDF:C\:\\Users\\marvi\\Zotero\\storage\\SMMRFDYG\\Knuth und Moore - 1975 - An analysis of alpha-beta pruning.pdf:application/pdf},
}

@article{vjekoslav_nemec_history_2019,
	title = {History {Of} {Chess} {Computer} {Engines}},
	url = {https://chessentials.com/history-of-chess-computer-engines/},
	urldate = {2024-11-23},
	author = {{Vjekoslav Nemec}},
	month = jan,
	year = {2019},
	file = {PDF:C\:\\Users\\marvi\\Zotero\\storage\\Q7KXM3SM\\Vjekoslav Nemec - 2019 - History Of Chess Computer Engines.pdf:application/pdf},
}

@article{baudet_branching_1978,
	title = {On the branching factor of the alpha-beta pruning algorithm},
	volume = {10},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370278800113},
	doi = {https://doi.org/10.1016/S0004-3702(78)80011-3},
	abstract = {An analysis of the alpha-beta pruning algorithm is presented which takes into account both shallow and deep cut-offs. A formula is first developed to measure the average number of terminal nodes examined by the algorithm in a uniform tree of degree n and depth d when ties are allowed among the bottom positions: specifically, all bottom values are assumed to be independent identically distributed random variables drawn from a discrete probability distribution. A worst case analysis over all possible probability distributions is then presented by considering the limiting case when the discrete probability distribution tends to a continuous probability distribution. The branching factor of the alpha-beta pruning algorithm is shown to grow with n as Θ(n/lnn), therefore confirming a claim by Knuth and Moore that deep cut-offs only have a second order effect on the behavior of the algorithm.},
	number = {2},
	journal = {Artificial Intelligence},
	author = {Baudet, Gérard M.},
	year = {1978},
	pages = {173--199},
	file = {PDF:C\:\\Users\\marvi\\Zotero\\storage\\NMIMNRWQ\\Baudet - 1978 - On the branching factor of the alpha-beta pruning algorithm.pdf:application/pdf},
}

@misc{silver_mastering_2017,
	title = {Mastering {Chess} and {Shogi} by {Self}-{Play} with a {General} {Reinforcement} {Learning} {Algorithm}},
	url = {https://arxiv.org/abs/1712.01815},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	year = {2017},
	note = {\_eprint: 1712.01815},
	file = {PDF:C\:\\Users\\marvi\\Zotero\\storage\\K225C95C\\Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.pdf:application/pdf},
}

@article{silver_mastering_2017-1,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/nature24270},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
	number = {7676},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	pages = {354--359},
	file = {PDF:C\:\\Users\\marvi\\Zotero\\storage\\UAMKNAVD\\Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf},
}

@article{browne_survey_2012,
	title = {A {Survey} of {Monte} {Carlo} {Tree} {Search} {Methods}},
	volume = {4},
	doi = {10.1109/TCIAIG.2012.2186810},
	number = {1},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	author = {Browne, Cameron B. and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M. and Cowling, Peter I. and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
	year = {2012},
	keywords = {Artificial intelligence, Artificial intelligence (AI), bandit-based methods, computer Go, Computers, Decision theory, game search, Game theory, Games, Markov processes, Monte Carlo methods, Monte Carlo tree search (MCTS), upper confidence bounds (UCB), upper confidence bounds for trees (UCT)},
	pages = {1--43},
	file = {PDF:C\:\\Users\\marvi\\Zotero\\storage\\49PRPGBZ\\Browne et al. - 2012 - A Survey of Monte Carlo Tree Search Methods.pdf:application/pdf},
}

@article{stefan_meyer-kahlen_universal_2025,
	title = {Universal {Chess} {Interface} ({UCI})},
	url = {https://www.shredderchess.com/chess-features/uci-universal-chess-interface.html},
	urldate = {2025-01-02},
	author = {{Stefan Meyer-Kahlen}},
	year = {2025},
	file = {engine-interface:C\:\\Users\\marvi\\Zotero\\storage\\N8EWGVGB\\engine-interface.txt:text/plain},
}

@article{wikpedia_foundation_inc_universal_2024,
	title = {Universal {Chess} {Interface}},
	url = {https://en.wikipedia.org/wiki/Universal_Chess_Interface},
	urldate = {2025-01-17},
	author = {{Wikpedia Foundation Inc.}},
	month = nov,
	year = {2024},
	file = {PDF:C\:\\Users\\marvi\\Zotero\\storage\\SVHJWSVT\\Wikpedia Foundation Inc. - 2024 - Universal Chess Interface.pdf:application/pdf},
}

@book{de_api_2017,
	address = {Berkeley, CA},
	title = {{API} {Management}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4842-1306-3 978-1-4842-1305-6},
	url = {http://link.springer.com/10.1007/978-1-4842-1305-6},
	language = {en},
	urldate = {2025-01-17},
	publisher = {Apress},
	author = {De, Brajesh},
	year = {2017},
	doi = {10.1007/978-1-4842-1305-6},
	file = {PDF:C\:\\Users\\marvi\\Zotero\\storage\\KGMMSBI3\\De - 2017 - API Management.pdf:application/pdf},
}

@article{tabibian_enhancing_2019,
	title = {Enhancing human learning via spaced repetition optimization},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1815156116},
	doi = {10.1073/pnas.1815156116},
	abstract = {Spaced repetition is a technique for efficient memorization which uses repeated review of content following a schedule determined by a spaced repetition algorithm to improve long-term retention. However, current spaced repetition algorithms are simple rule-based heuristics with a few hard-coded parameters. Here, we introduce a flexible representation of spaced repetition using the framework of marked temporal point processes and then address the design of spaced repetition algorithms with provable guarantees as an optimal control problem for stochastic differential equations with jumps. For two well-known human memory models, we show that, if the learner aims to maximize recall probability of the content to be learned subject to a cost on the reviewing frequency, the optimal reviewing schedule is given by the recall probability itself. As a result, we can then develop a simple, scalable online spaced repetition algorithm, MEMORIZE, to determine the optimal reviewing times. We perform a large-scale natural experiment using data from Duolingo, a popular language-learning online platform, and show that learners who follow a reviewing schedule determined by our algorithm memorize more effectively than learners who follow alternative schedules determined by several heuristics.},
	language = {en},
	number = {10},
	urldate = {2025-01-24},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Tabibian, Behzad and Upadhyay, Utkarsh and De, Abir and Zarezade, Ali and Schölkopf, Bernhard and Gomez-Rodriguez, Manuel},
	month = mar,
	year = {2019},
	pages = {3988--3993},
	file = {Volltext:C\:\\Users\\marvi\\Zotero\\storage\\HFFJ3K4Q\\Tabibian et al. - 2019 - Enhancing human learning via spaced repetition optimization.pdf:application/pdf},
}

@inproceedings{reddy_unbounded_2016,
	address = {San Francisco California USA},
	title = {Unbounded {Human} {Learning}: {Optimal} {Scheduling} for {Spaced} {Repetition}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {Unbounded {Human} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939850},
	doi = {10.1145/2939672.2939850},
	language = {en},
	urldate = {2025-01-24},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Reddy, Siddharth and Labutov, Igor and Banerjee, Siddhartha and Joachims, Thorsten},
	month = aug,
	year = {2016},
	pages = {1815--1824},
	file = {Volltext:C\:\\Users\\marvi\\Zotero\\storage\\QHQAL9NU\\Reddy et al. - 2016 - Unbounded Human Learning Optimal Scheduling for Spaced Repetition.pdf:application/pdf},
}

@article{sojka_performance_2022,
	title = {Performance comparison between selected chess engines},
	volume = {24},
	doi = {10.35784/jcsi.2975},
	journal = {Journal of Computer Sciences Institute},
	author = {Sojka, Maciej},
	month = sep,
	year = {2022},
	pages = {228--235},
	file = {PDF:C\:\\Users\\marvi\\Zotero\\storage\\UN5JXR2N\\Sojka - 2022 - Performance comparison between selected chess engines.pdf:application/pdf},
}

@article{haworth_20th_nodate,
	title = {The 20th {Top} {Chess} {Engine} {Championship}: {TCEC20}},
	language = {en},
	author = {Haworth, Guy and Hernandez, Nelson},
	file = {PDF:C\:\\Users\\marvi\\Zotero\\storage\\4WBQ9SXF\\Haworth und Hernandez - The 20th Top Chess Engine Championship TCEC20.pdf:application/pdf},
}

@article{tcec_chessdom_tcec-chess_2025,
	title = {{TCEC}-{Chess}},
	url = {https://tcec-chess.com/},
	urldate = {2025-01-27},
	author = {{TCEC Chessdom}},
	year = {2025},
}

@article{noauthor_neural_2020,
	title = {Neural network topology},
	url = {https://lczero.org/dev/backend/nn/},
	urldate = {2025-01-27},
	month = nov,
	year = {2020},
	file = {PDF:C\:\\Users\\marvi\\Zotero\\storage\\2KGHMJG6\\Neural network topology.pdf:application/pdf},
}

@article{wikpedia_foundation_inc_stockfish_2025,
	title = {Stockfish (chess)},
	url = {https://en.wikipedia.org/wiki/Stockfish_(chess)},
	urldate = {2025-01-31},
	author = {{Wikpedia Foundation Inc.}},
	month = jan,
	year = {2025},
}

@article{wikpedia_foundation_inc_komodo_2024,
	title = {Komodo ({Schach})},
	url = {https://de.wikipedia.org/wiki/Komodo_(Schach)},
	urldate = {2025-01-31},
	author = {{Wikpedia Foundation Inc.}},
	month = sep,
	year = {2024},
}
